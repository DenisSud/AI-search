{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10744015,
          "sourceType": "datasetVersion",
          "datasetId": 6662849
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate base model on MMLU and MMLU_RU"
      ],
      "metadata": {
        "id": "lu4exjWkt-nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Imports"
      ],
      "metadata": {
        "id": "D7Ot451XuJ0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm.notebook import tqdm\n",
        "import torch_xla.core.xla_model as xm # This should now work"
      ],
      "metadata": {
        "id": "r97sjxyouHy2",
        "outputId": "8f947ecd-bca6-4ab7-9843-a857875011f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'XLA_LIB' from partially initialized module 'torch_xla.core.xla_model' (most likely due to a circular import) (/usr/local/lib/python3.11/dist-packages/torch_xla/core/xla_model.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-86676f37985f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxm\u001b[0m \u001b[0;31m# This should now work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/core/xla_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_xla\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mruntime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_env_vars\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstablehlo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_as_stablehlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_torch_model_as_stablehlo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplugins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/stablehlo.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m from torch_xla.experimental.stablehlo_custom_call import (\n\u001b[1;32m     22\u001b[0m     extract_custom_call_outputs_shape_dtype, stablehlo_custom_call)\n\u001b[0;32m---> 23\u001b[0;31m from torch_xla.experimental.unbounded_dynamism_export import (\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mexported_program_has_symbolic_input_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     process_exported_program_with_symbolic_input)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/experimental/unbounded_dynamism_export.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_dynamic_reshape_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inductor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_fake_args_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/experimental/xla_dynamic_reshape_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLibrary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXLA_LIB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m XLA_LIB.define(\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'XLA_LIB' from partially initialized module 'torch_xla.core.xla_model' (most likely due to a circular import) (/usr/local/lib/python3.11/dist-packages/torch_xla/core/xla_model.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading"
      ],
      "metadata": {
        "id": "U-KPI3g4uR6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name):\n",
        "    \"\"\"Load model and tokenizer with TPU support\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    model.eval()\n",
        "    # Move model to TPU\n",
        "    device = xm.xla_device()\n",
        "    model = model.to(device)\n",
        "    return model, tokenizer, device"
      ],
      "metadata": {
        "id": "kPJ9cKjpuBkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "-Bo5H8fquUvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mmlu_data(subjects=None):\n",
        "    \"\"\"Load MMLU-RU test data for specified subjects\"\"\"\n",
        "    if subjects is None:\n",
        "        # You can modify this list to include only subjects you want to evaluate\n",
        "        subjects = [\n",
        "            'abstract_algebra',\n",
        "            'college_mathematics',\n",
        "            'machine_learning',\n",
        "            'college_physics'\n",
        "        ]\n",
        "\n",
        "    dfs = []\n",
        "    for subject in subjects:\n",
        "        try:\n",
        "            dataset = load_dataset(\"NLPCoreTeam/mmlu_ru\", subject, split=\"test\")\n",
        "            df = dataset.to_pandas()\n",
        "            df['subject'] = subject\n",
        "            dfs.append(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {subject}: {e}\")\n",
        "\n",
        "    return pd.concat(dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "3Hokmy2UuUSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Functions"
      ],
      "metadata": {
        "id": "WJtNM3CIufWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt(row):\n",
        "    \"\"\"Format a single question into a prompt\"\"\"\n",
        "    prompt = f\"Question: {row['question_ru']}\\nChoices:\\n\"\n",
        "    for idx, choice in enumerate(row['choices_ru']):\n",
        "        prompt += f\"{chr(65 + idx)}. {choice}\\n\"\n",
        "    prompt += \"Answer:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def evaluate_model(model, tokenizer, df, device):\n",
        "    \"\"\"Evaluate model on the dataset\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        prompt = format_prompt(row)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=5,\n",
        "                temperature=0.0,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated = response[len(prompt):].strip()\n",
        "\n",
        "        # Extract first capital letter as prediction\n",
        "        pred = next((c for c in generated if c.upper() in 'ABCD'), 'X')\n",
        "\n",
        "        results.append({\n",
        "            'subject': row['subject'],\n",
        "            'question': row['question_ru'],\n",
        "            'correct_answer': row['answer'],\n",
        "            'predicted_answer': pred,\n",
        "            'correct': pred == row['answer']\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n"
      ],
      "metadata": {
        "id": "hMeaRkKfufAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load model\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"  # e.g., \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "model, tokenizer, device = load_model_and_tokenizer(model_name)"
      ],
      "metadata": {
        "id": "2D-HX9nIunTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load data\n",
        "eval_df = load_mmlu_data()"
      ],
      "metadata": {
        "id": "7I45Xt8cuwfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Run evaluation\n",
        "results_df = evaluate_model(model, tokenizer, eval_df, device)"
      ],
      "metadata": {
        "id": "Exv4Vi2BuxyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Calculate and display results\n",
        "accuracy = results_df['correct'].mean()\n",
        "subject_accuracy = results_df.groupby('subject')['correct'].mean()\n",
        "\n",
        "print(f\"Overall accuracy: {accuracy:.2%}\")\n",
        "print(\"\\nAccuracy by subject:\")\n",
        "print(subject_accuracy)"
      ],
      "metadata": {
        "id": "t1IvwDeUuzJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Save results\n",
        "results_df.to_csv(f\"mmlu_results_{model_name.replace('/', '_')}.csv\", index=False)"
      ],
      "metadata": {
        "id": "ntG5u8Acu0R_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}